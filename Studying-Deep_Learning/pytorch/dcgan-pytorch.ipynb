{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom PIL import Image\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torchvision  \nfrom torchvision import models, transforms\nfrom tqdm import tqdm","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# resize 32*32","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(nn.Module):\n    \n    def __init__(self, z_dim=20, image_size=64):\n        super(Generator, self).__init__()\n        \n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, image_size*8, kernel_size=4, stride=1),\n            nn.BatchNorm2d(image_size*8),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(image_size*8, image_size*4, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(image_size*4),\n            nn.ReLU(inplace=True)\n        )\n            \n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(image_size*4, image_size*2, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(image_size*2),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(image_size*2, image_size, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(image_size),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.last = nn.Sequential(\n            nn.ConvTranspose2d(image_size, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n        \n    def forward(self, z):\n        out = self.layer1(z)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n            \n        return out","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Discriminator(nn.Module):\n    \n    def __init__(self, z_dim=20, image_size=64):\n            super(Discriminator, self).__init__()\n            \n            self.layer1 = nn.Sequential(\n                nn.Conv2d(3, image_size, kernel_size=4, stride=2, padding=1),\n                nn.LeakyReLU(0.1, inplace=True)\n            )\n            \n            self.layer2 = nn.Sequential(\n                nn.Conv2d(image_size, image_size*2, kernel_size=4, stride=2, padding=1),\n                nn.LeakyReLU(0.1, inplace=True)\n            )\n            \n            self.layer3 = nn.Sequential(\n                nn.Conv2d(image_size*2, image_size*4, kernel_size=4, stride=2, padding=1),\n                nn.LeakyReLU(0.1, inplace=True)\n            )\n            \n            self.layer4 = nn.Sequential(\n                nn.Conv2d(image_size*4, image_size*8, kernel_size=4, stride=2, padding=1),\n                nn.LeakyReLU(0.1, inplace=True)\n            )\n            \n            self.last = nn.Sequential(\n                nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1),\n                nn.Sigmoid()\n            )\n            \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        \n        return out.view(-1, 1).squeeze(1)\n                ","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_datapath_list():\n    \n    train_img_list = list()\n    \n    for img_idx in range(1,3000,1):\n        img_path = '/kaggle/input/cat-and-dog/training_set/training_set/cats/cat.'+ str(img_idx)+'.jpg'\n        train_img_list.append(img_path)\n        \n    return train_img_list\n\n\nclass ImageTransform():\n    \n    def __init__(self, mean, std):\n        self.data_trandform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n        \n    def __call__(self, img):\n        return self.data_trandform(img)\n    \nclass GAN_Dataset(data.Dataset):\n    \n    def __init__(self, file_list, transform):\n        self.file_list = file_list\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, idx):\n        img_path = self.file_list[idx]\n        img = Image.open(img_path)\n        img_resize = img.resize((64, 64))\n        \n        img_transformed = self.transform(img_resize)\n        \n        return img_transformed","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_img_list = make_datapath_list()\n\nmean = (0.5, )\nstd = (0.5, )\nbatch_size = 64\n\ntrain_dataset = GAN_Dataset(train_img_list, ImageTransform(mean, std))\ntrain_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G = Generator(z_dim=20, image_size=32)\nD = Discriminator(z_dim=20, image_size=32)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\nG.apply(weights_init)\nD.apply(weights_init)","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"Discriminator(\n  (layer1): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n  )\n  (layer2): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n  )\n  (layer3): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n  )\n  (layer4): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n  )\n  (last): Sequential(\n    (0): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1))\n    (1): Sigmoid()\n  )\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(G, D, dataloader, num_epochs):\n\n    # GPUが使えるかを確認\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"使用デバイス：\", device)\n    \n    # ラベルの定義（本物:1, 偽物:0）\n    real_label = 1\n    fake_label = 0\n\n    # 最適化手法の設定\n    g_lr, d_lr = 0.0001, 0.0004\n    beta1, beta2 = 0.0, 0.9\n    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2])\n    d_optimizer = torch.optim.Adam(D.parameters(), d_lr, [beta1, beta2])\n\n    # 誤差関数を定義\n    criterion = nn.BCELoss()\n\n    # パラメータをハードコーディング\n    z_dim = 20\n    mini_batch_size = 64\n\n    # ネットワークをGPUへ\n    G.to(device)\n    D.to(device)\n\n    G.train()  # モデルを訓練モードに\n    D.train()  # モデルを訓練モードに\n\n    # ネットワークがある程度固定であれば、高速化させる\n    torch.backends.cudnn.benchmark = True\n\n    # 画像の枚数\n    num_train_imgs = len(dataloader.dataset)\n    batch_size = dataloader.batch_size\n\n    # イテレーションカウンタをセット\n    iteration = 1\n    logs = []\n\n    # epochのループ\n    for epoch in range(num_epochs):\n\n        epoch_g_loss = 0.0  # epochの損失和\n        epoch_d_loss = 0.0  # epochの損失和\n\n        print('-------------')\n        print('Epoch {}/{}'.format(epoch, num_epochs))\n        print('-------------')\n        print('（train）')\n\n        # データローダーからminibatchずつ取り出すループ\n        for imgs in dataloader:\n            # Discriminatorの学習\n            if imgs.size()[0] == 1:\n                continue\n\n            imgs = imgs.to(device)\n\n            # 正解ラベルと偽ラベルを作成\n            mini_batch_size = imgs.size()[0]\n            label_real = torch.full((mini_batch_size,), fill_value=real_label, dtype=torch.float, device=device)\n            label_fake = torch.full((mini_batch_size,), fill_value=fake_label, dtype=torch.float, device=device)\n\n            # 真の画像を判定\n            d_out_real = D(imgs)\n\n            # 偽の画像を生成して判定\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n            fake_imags = G(input_z)\n            d_out_fake = D(fake_imags)\n\n            # 誤差を計算\n            d_loss_real = criterion(d_out_real, label_real)\n            d_loss_fake = criterion(d_out_fake, label_fake)\n            d_loss = d_loss_real + d_loss_fake\n\n            # バックプロパゲーション\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n\n            d_loss.backward()\n            d_optimizer.step()\n\n            # Generatorの学習\n            # 偽の画像を生成\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images)\n\n            # 誤差\n            g_loss = criterion(d_out_fake, label_real)\n\n            # バックプロパゲーション\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n            g_loss.backward()\n            g_optimizer.step()\n\n            # 記録\n            epoch_d_loss += d_loss.item()\n            epoch_g_loss += g_loss.item()\n            iteration += 1\n\n        # epochのphaseごとのlossと正解率\n        print('-------------')\n        print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f}'.format(\n            epoch, epoch_d_loss/batch_size, epoch_g_loss/batch_size))\n\n    return G, D","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 150\nG_update, D_update = train_model(G, D, dataloader=train_dataloader, num_epochs=num_epochs)","execution_count":null,"outputs":[{"output_type":"stream","text":"使用デバイス： cuda:0\n-------------\nEpoch 0/150\n-------------\n（train）\n-------------\nepoch 0 || Epoch_D_Loss:0.3074 ||Epoch_G_Loss:2.1382\n-------------\nEpoch 1/150\n-------------\n（train）\n-------------\nepoch 1 || Epoch_D_Loss:0.1993 ||Epoch_G_Loss:2.5707\n-------------\nEpoch 2/150\n-------------\n（train）\n-------------\nepoch 2 || Epoch_D_Loss:0.2875 ||Epoch_G_Loss:2.6366\n-------------\nEpoch 3/150\n-------------\n（train）\n-------------\nepoch 3 || Epoch_D_Loss:0.3171 ||Epoch_G_Loss:2.5466\n-------------\nEpoch 4/150\n-------------\n（train）\n-------------\nepoch 4 || Epoch_D_Loss:0.6079 ||Epoch_G_Loss:1.4271\n-------------\nEpoch 5/150\n-------------\n（train）\n-------------\nepoch 5 || Epoch_D_Loss:0.6346 ||Epoch_G_Loss:1.2776\n-------------\nEpoch 6/150\n-------------\n（train）\n-------------\nepoch 6 || Epoch_D_Loss:0.4739 ||Epoch_G_Loss:1.5952\n-------------\nEpoch 7/150\n-------------\n（train）\n-------------\nepoch 7 || Epoch_D_Loss:0.4647 ||Epoch_G_Loss:1.7563\n-------------\nEpoch 8/150\n-------------\n（train）\n-------------\nepoch 8 || Epoch_D_Loss:0.5268 ||Epoch_G_Loss:1.6056\n-------------\nEpoch 9/150\n-------------\n（train）\n-------------\nepoch 9 || Epoch_D_Loss:0.6160 ||Epoch_G_Loss:1.4862\n-------------\nEpoch 10/150\n-------------\n（train）\n-------------\nepoch 10 || Epoch_D_Loss:0.6219 ||Epoch_G_Loss:1.4697\n-------------\nEpoch 11/150\n-------------\n（train）\n-------------\nepoch 11 || Epoch_D_Loss:0.6816 ||Epoch_G_Loss:1.1823\n-------------\nEpoch 12/150\n-------------\n（train）\n-------------\nepoch 12 || Epoch_D_Loss:0.7179 ||Epoch_G_Loss:1.1046\n-------------\nEpoch 13/150\n-------------\n（train）\n-------------\nepoch 13 || Epoch_D_Loss:0.7936 ||Epoch_G_Loss:1.0643\n-------------\nEpoch 14/150\n-------------\n（train）\n-------------\nepoch 14 || Epoch_D_Loss:0.7778 ||Epoch_G_Loss:0.9876\n-------------\nEpoch 15/150\n-------------\n（train）\n-------------\nepoch 15 || Epoch_D_Loss:0.8574 ||Epoch_G_Loss:0.8796\n-------------\nEpoch 16/150\n-------------\n（train）\n-------------\nepoch 16 || Epoch_D_Loss:0.8486 ||Epoch_G_Loss:0.9026\n-------------\nEpoch 17/150\n-------------\n（train）\n-------------\nepoch 17 || Epoch_D_Loss:0.8489 ||Epoch_G_Loss:0.9366\n-------------\nEpoch 18/150\n-------------\n（train）\n-------------\nepoch 18 || Epoch_D_Loss:0.8423 ||Epoch_G_Loss:0.9430\n-------------\nEpoch 19/150\n-------------\n（train）\n-------------\nepoch 19 || Epoch_D_Loss:0.8623 ||Epoch_G_Loss:0.8360\n-------------\nEpoch 20/150\n-------------\n（train）\n-------------\nepoch 20 || Epoch_D_Loss:0.8557 ||Epoch_G_Loss:0.8690\n-------------\nEpoch 21/150\n-------------\n（train）\n-------------\nepoch 21 || Epoch_D_Loss:0.8625 ||Epoch_G_Loss:0.9075\n-------------\nEpoch 22/150\n-------------\n（train）\n-------------\nepoch 22 || Epoch_D_Loss:0.8229 ||Epoch_G_Loss:0.9572\n-------------\nEpoch 23/150\n-------------\n（train）\n-------------\nepoch 23 || Epoch_D_Loss:0.7973 ||Epoch_G_Loss:0.9922\n-------------\nEpoch 24/150\n-------------\n（train）\n-------------\nepoch 24 || Epoch_D_Loss:0.7917 ||Epoch_G_Loss:1.0542\n-------------\nEpoch 25/150\n-------------\n（train）\n-------------\nepoch 25 || Epoch_D_Loss:0.7697 ||Epoch_G_Loss:1.1140\n-------------\nEpoch 26/150\n-------------\n（train）\n-------------\nepoch 26 || Epoch_D_Loss:0.7558 ||Epoch_G_Loss:1.1722\n-------------\nEpoch 27/150\n-------------\n（train）\n-------------\nepoch 27 || Epoch_D_Loss:0.7553 ||Epoch_G_Loss:1.1158\n-------------\nEpoch 28/150\n-------------\n（train）\n-------------\nepoch 28 || Epoch_D_Loss:0.7288 ||Epoch_G_Loss:1.1471\n-------------\nEpoch 29/150\n-------------\n（train）\n-------------\nepoch 29 || Epoch_D_Loss:0.7293 ||Epoch_G_Loss:1.1760\n-------------\nEpoch 30/150\n-------------\n（train）\n-------------\nepoch 30 || Epoch_D_Loss:0.6906 ||Epoch_G_Loss:1.2066\n-------------\nEpoch 31/150\n-------------\n（train）\n-------------\nepoch 31 || Epoch_D_Loss:0.6834 ||Epoch_G_Loss:1.2441\n-------------\nEpoch 32/150\n-------------\n（train）\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# 生成画像と訓練データを可視化する\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# 入力の乱数生成\nbatch_size = 8\ndim = 20\nz = torch.randn(batch_size, dim)\nz = z.view(z.size(0), z.size(1), 1, 1)\n\n# 画像生成\nG_update.eval()\nfake_imags = G_update(z.to(device))\n\n# 訓練データ\nbatch_iterator = iter(train_dataloader) \nimgs = next(batch_iterator)  \n\n\n# 出力\nfig = plt.figure(figsize=(15, 6))\nfor i in range(0, 5):\n    # 上段に訓練データを\n    plt.subplot(2, 5, i+1)\n    plt.imshow(imgs[i][0].cpu().detach().numpy())\n\n    # 下段に生成データを表示する\n    plt.subplot(2, 5, 5+i+1)\n    plt.imshow(fake_imags[i][0].cpu().detach().numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}