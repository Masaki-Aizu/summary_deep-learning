{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 73s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.datasets import cifar10\n",
    "(image_train, label_train), (image_test, label_test) = \\\n",
    "                cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img, label for training (50000, 32, 32, 3) (50000, 1)\n",
      "img, label for test (10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('img, label for training', image_train.shape, label_train.shape)\n",
    "print('img, label for test', image_test.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = image_train / 255\n",
    "image_test = image_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import to_categorical\n",
    "label_train = to_categorical(label_train, 10)\n",
    "label_test = to_categorical(label_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/masaki_endo/anaconda3/envs/my_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Conv2D\n",
    "from tensorflow.python.keras.layers import MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(\n",
    "    Conv2D(\n",
    "    filters=32,\n",
    "    input_shape=(32,32,3),\n",
    "    kernel_size=(3,3),\n",
    "    strides=(1,1),\n",
    "    padding='same',\n",
    "    activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    MaxPooling2D(\n",
    "    pool_size=(2,2)\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    MaxPooling2D(\n",
    "     pool_size=(2,2)\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(\n",
    "    Dense(\n",
    "        units=512,\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(\n",
    "    Dense(\n",
    "        units=10,\n",
    "        activation='softmax'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 613,994\n",
      "Trainable params: 613,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "40000/40000 [==============================] - 29s 721us/sample - loss: 1.7150 - acc: 0.3652 - val_loss: 1.3576 - val_acc: 0.4990\n",
      "Epoch 2/3\n",
      "40000/40000 [==============================] - 28s 705us/sample - loss: 1.3414 - acc: 0.5119 - val_loss: 1.1890 - val_acc: 0.5763\n",
      "Epoch 3/3\n",
      "40000/40000 [==============================] - 28s 702us/sample - loss: 1.1899 - acc: 0.5732 - val_loss: 1.0606 - val_acc: 0.6149\n"
     ]
    }
   ],
   "source": [
    "CNNmodel1=model.fit(\n",
    "    image_train,\n",
    "    label_train,\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    validation_split = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model.save('myCNN_3epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict(image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2846308e-04, 2.0949161e-04, 1.1726785e-02, 8.3533865e-01,\n",
       "       2.5885801e-03, 1.1572691e-01, 2.7571972e-02, 4.5555211e-03,\n",
       "       1.7325867e-03, 3.2102919e-04], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from keras.preprocessing.image import load_img, img_to_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesU = [filename for filename in listdir('./chapter4_dataset/Directions01_RGB/train/up')\n",
    "         if not filename.startswith('.')]\n",
    "filesD = [filename for filename in listdir('./chapter4_dataset/Directions01_RGB/train/down')\n",
    "         if not filename.startswith('.')]\n",
    "filesL = [filename for filename in listdir('./chapter4_dataset/Directions01_RGB/train/left')\n",
    "         if not filename.startswith('.')]\n",
    "filesR = [filename for filename in listdir('./chapter4_dataset/Directions01_RGB/train/right')\n",
    "         if not filename.startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['228.png', '15.png', '0.png', '165.png', '88.png', '234.png', '6.png', '171.png', '191.png', '118.png', '30.png', '46.png', '202.png', '38.png', '101.png', '197.png', '179.png', '223.png', '59.png', '120.png', '226.png', '220.png', '21.png', '27.png', '203.png', '54.png', '188.png', '33.png', '20.png', '123.png', '22.png', '183.png', '44.png', '128.png', '55.png', '90.png', '96.png', '131.png', '141.png', '67.png', '83.png', '11.png', '12.png', '121.png', '91.png', '61.png', '78.png', '180.png', '73.png', '204.png', '224.png', '193.png', '52.png', '116.png', '108.png', '103.png', '35.png', '93.png', '206.png', '65.png', '82.png', '134.png', '122.png', '10.png', '42.png', '140.png', '160.png', '182.png', '3.png', '214.png', '40.png', '25.png', '133.png', '26.png', '31.png', '227.png', '161.png', '146.png', '125.png', '7.png', '211.png', '153.png', '17.png', '181.png', '29.png', '218.png', '1.png', '34.png', '149.png', '207.png', '47.png', '110.png', '167.png', '39.png', '145.png', '163.png', '63.png', '85.png', '229.png', '177.png', '58.png', '151.png', '19.png', '49.png', '147.png', '155.png', '205.png', '13.png', '235.png', '176.png', '113.png', '23.png', '4.png', '143.png', '107.png', '56.png', '64.png', '129.png', '186.png', '166.png', '66.png', '174.png', '70.png', '112.png', '60.png', '106.png', '94.png', '28.png', '192.png', '164.png', '219.png', '80.png', '152.png', '156.png', '57.png', '135.png', '210.png', '233.png', '117.png', '190.png', '32.png', '230.png', '79.png', '194.png', '198.png', '201.png', '43.png', '189.png', '8.png', '53.png', '185.png', '115.png', '200.png', '169.png', '212.png', '69.png', '50.png', '222.png', '16.png', '86.png', '102.png', '208.png', '217.png', '36.png', '111.png', '157.png', '119.png', '74.png', '92.png', '199.png', '175.png', '62.png', '213.png', '142.png', '196.png', '150.png', '81.png', '48.png', '168.png', '89.png', '195.png', '162.png', '209.png', '127.png', '124.png', '109.png', '159.png', '104.png', '136.png', '138.png', '187.png', '154.png', '97.png', '37.png', '126.png', '98.png', '77.png', '130.png', '173.png', '76.png', '95.png', '170.png', '144.png', '236.png', '178.png', '216.png', '5.png', '75.png', '114.png', '87.png', '184.png', '137.png', '225.png', '51.png', '100.png', '45.png', '172.png', '84.png', '148.png', '72.png', '71.png', '2.png', '139.png', '221.png', '231.png', '99.png', '132.png', '9.png', '24.png', '18.png', '14.png', '105.png', '232.png', '68.png', '158.png', '41.png', '215.png']\n"
     ]
    }
   ],
   "source": [
    "print(filesU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chestU = np.zeros((len(filesU), 32, 32, 3))\n",
    "chestD = np.zeros((len(filesD), 32, 32, 3))\n",
    "chestL = np.zeros((len(filesL), 32, 32, 3))\n",
    "chestR = np.zeros((len(filesR), 32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(filesU)):\n",
    "    fnameform = './chapter4_dataset/Directions01_RGB/train/up/%s'%filesU[i]\n",
    "    chestU[i] = np.array(img_to_array(load_img(fnameform, target_size=(32,32))))/ 255\n",
    "    \n",
    "for i in range(len(filesL)):\n",
    "    fnameform = './chapter4_dataset/Directions01_RGB/train/left/%s'%filesL[i]\n",
    "    chestL[i] = np.array(img_to_array(load_img(fnameform, target_size=(32,32))))/255\n",
    "    \n",
    "for i in range(len(filesD)):\n",
    "    fnameform = './chapter4_dataset/Directions01_RGB/train/down/%s'%filesD[i]\n",
    "    chestD[i] = np.array(img_to_array(load_img(fnameform, target_size=(32,32))))/255\n",
    "    \n",
    "for i in range(len(filesR)):\n",
    "    fnameform = './chapter4_dataset/Directions01_RGB/train/right/%s'%filesR[i]\n",
    "    chestR[i] = np.array(img_to_array(load_img(fnameform, target_size=(32,32))))/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(chestU.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chestimg_train = np.concatenate((chestU, chestD, chestL, chestR), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chestlabelU = np.full((len(filesU), 1), 0, dtype='uint8')\n",
    "chestlabelD = np.full((len(filesD), 1), 1, dtype='uint8')\n",
    "chestlabelL = np.full((len(filesL), 1), 2, dtype='uint8')\n",
    "chestlabelR = np.full((len(filesR), 1), 3, dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chestlabel_train = np.concatenate((chestlabelU, chestlabelD, \n",
    "                                   chestlabelL, chestlabelR), axis=0)\n",
    "chestonehotlabel_train = to_categorical(chestlabel_train, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chestonehotlabel_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Conv2D\n",
    "from tensorflow.python.keras.layers import MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 2,165,284\n",
      "Trainable params: 2,165,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        input_shape=(32,32,3),\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    ) \n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3,3),\n",
    "        strides = (1,1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(\n",
    "    Dense(\n",
    "        units=512, activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(\n",
    "    Dense(\n",
    "        units=4, activation='softmax'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 758 samples, validate on 190 samples\n",
      "Epoch 1/10\n",
      "758/758 [==============================] - 1s 894us/sample - loss: 15.9018 - acc: 0.3681 - val_loss: 2.2772 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "758/758 [==============================] - 1s 910us/sample - loss: 0.4444 - acc: 0.8588 - val_loss: 0.3063 - val_acc: 0.9211\n",
      "Epoch 3/10\n",
      "758/758 [==============================] - 1s 1ms/sample - loss: 0.0386 - acc: 0.9894 - val_loss: 0.0225 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "758/758 [==============================] - 1s 1ms/sample - loss: 0.0112 - acc: 0.9974 - val_loss: 4.5682e-04 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "758/758 [==============================] - 1s 1ms/sample - loss: 0.0030 - acc: 0.9987 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "758/758 [==============================] - 1s 887us/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 5.0046e-04 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "758/758 [==============================] - 1s 902us/sample - loss: 6.2587e-04 - acc: 1.0000 - val_loss: 3.2976e-04 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "758/758 [==============================] - 1s 913us/sample - loss: 4.5891e-04 - acc: 1.0000 - val_loss: 3.6758e-04 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "758/758 [==============================] - 1s 898us/sample - loss: 0.0051 - acc: 0.9974 - val_loss: 5.3788e-04 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "758/758 [==============================] - 1s 904us/sample - loss: 0.0029 - acc: 0.9987 - val_loss: 0.0092 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model_histry = model.fit(\n",
    "               chestimg_train,\n",
    "               chestonehotlabel_train,\n",
    "               batch_size=32,\n",
    "               epochs=10,\n",
    "               validation_split=0.2\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s 226us/sample\n"
     ]
    }
   ],
   "source": [
    "results = list(model.predict_classes(chestimg_train, verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix = confusion_matrix(chestlabel_train, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[237,   0,   0,   0],\n",
       "       [  0, 237,   0,   0],\n",
       "       [  0,   0, 237,   0],\n",
       "       [  0,   0,   0, 237]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[237   0   0   0]\n",
      " [  0 237   0   0]\n",
      " [  0   0 237   0]\n",
      " [  0   0   0 237]]\n"
     ]
    }
   ],
   "source": [
    "print(cmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(chestlabel_train, results)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "gsdata = pd.read_csv('./chapter4_dataset/XPAge01_RGB/XP/trainingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filenames', 'age'], dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingImgdata = np.zeros((len(gsdata['filenames']), 128, 128, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 128, 128, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(gsdata['filenames'])):\n",
    "    fnameform = './chapter4_dataset/XPAge01_RGB/XP/JPGs/%s'%gsdata['filenames'][i]\n",
    "    trainingImgdata[i] = np.array(img_to_array(load_img(fnameform, grayscale=True, target_size=(128,128)))) / 255\n",
    "    \n",
    "trainingImgdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingGSdata = np.zeros((len(gsdata['age']), 1))\n",
    "\n",
    "for i in range(len(gsdata['filenames'])):\n",
    "    trainingGSdata[i][0] = gsdata['age'][i]\n",
    "    \n",
    "trainingGSdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingGS = trainingGSdata.copy()\n",
    "trainingGS = trainingGS.astype('int')\n",
    "# \n",
    "trainingGS = np.ravel(trainingGS)\n",
    "\n",
    "trainingGS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingGS = trainingGS /  100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingGS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Conv2D\n",
    "from tensorflow.python.keras.layers import MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 128, 128, 32)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               33554944  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 33,620,449\n",
      "Trainable params: 33,620,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        input_shape=(128,128,1),\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mse']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples, validate on 16 samples\n",
      "Epoch 1/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 686.7153 - mean_squared_error: 686.7154 - val_loss: 0.3405 - val_mean_squared_error: 0.3405\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 1s 22ms/sample - loss: 0.6048 - mean_squared_error: 0.6048 - val_loss: 0.1009 - val_mean_squared_error: 0.1009\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 1s 22ms/sample - loss: 0.1025 - mean_squared_error: 0.1025 - val_loss: 0.1718 - val_mean_squared_error: 0.1718\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0827 - mean_squared_error: 0.0827 - val_loss: 0.0876 - val_mean_squared_error: 0.0876\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0820 - mean_squared_error: 0.0820 - val_loss: 0.0923 - val_mean_squared_error: 0.0923\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 1s 22ms/sample - loss: 0.0903 - mean_squared_error: 0.0903 - val_loss: 0.0878 - val_mean_squared_error: 0.0878\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 1s 23ms/sample - loss: 0.0983 - mean_squared_error: 0.0983 - val_loss: 0.0938 - val_mean_squared_error: 0.0938\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 1s 22ms/sample - loss: 0.0502 - mean_squared_error: 0.0502 - val_loss: 0.0754 - val_mean_squared_error: 0.0754\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0666 - mean_squared_error: 0.0666 - val_loss: 0.1080 - val_mean_squared_error: 0.1080\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0603 - mean_squared_error: 0.0603 - val_loss: 0.0723 - val_mean_squared_error: 0.0723\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 1s 22ms/sample - loss: 0.0567 - mean_squared_error: 0.0567 - val_loss: 0.0908 - val_mean_squared_error: 0.0908\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 1s 23ms/sample - loss: 0.0618 - mean_squared_error: 0.0618 - val_loss: 0.0741 - val_mean_squared_error: 0.0741\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0703 - mean_squared_error: 0.0703 - val_loss: 0.0978 - val_mean_squared_error: 0.0978\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0625 - mean_squared_error: 0.0625 - val_loss: 0.0856 - val_mean_squared_error: 0.0856\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0750 - mean_squared_error: 0.0750 - val_loss: 0.0757 - val_mean_squared_error: 0.0757\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0623 - mean_squared_error: 0.0623 - val_loss: 0.1068 - val_mean_squared_error: 0.1068\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0669 - mean_squared_error: 0.0669 - val_loss: 0.0981 - val_mean_squared_error: 0.0981\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0625 - mean_squared_error: 0.0625 - val_loss: 0.0776 - val_mean_squared_error: 0.0776\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 0.0781 - val_mean_squared_error: 0.0781\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0776 - mean_squared_error: 0.0776 - val_loss: 0.0933 - val_mean_squared_error: 0.0933\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0561 - mean_squared_error: 0.0561 - val_loss: 0.0584 - val_mean_squared_error: 0.0584\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0705 - mean_squared_error: 0.0705 - val_loss: 0.1151 - val_mean_squared_error: 0.1151\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0762 - mean_squared_error: 0.0762 - val_loss: 0.1158 - val_mean_squared_error: 0.1158\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - 1s 23ms/sample - loss: 0.0651 - mean_squared_error: 0.0651 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0595 - mean_squared_error: 0.0595 - val_loss: 0.0969 - val_mean_squared_error: 0.0969\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - 1s 22ms/sample - loss: 0.0911 - mean_squared_error: 0.0911 - val_loss: 0.0736 - val_mean_squared_error: 0.0736\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0716 - mean_squared_error: 0.0716 - val_loss: 0.1087 - val_mean_squared_error: 0.1087\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - 1s 22ms/sample - loss: 0.0732 - mean_squared_error: 0.0732 - val_loss: 0.0603 - val_mean_squared_error: 0.0603\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0707 - mean_squared_error: 0.0707 - val_loss: 0.0975 - val_mean_squared_error: 0.0975\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0686 - mean_squared_error: 0.0686 - val_loss: 0.0982 - val_mean_squared_error: 0.0982\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0707 - mean_squared_error: 0.0707 - val_loss: 0.1125 - val_mean_squared_error: 0.1125\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0523 - mean_squared_error: 0.0523 - val_loss: 0.0917 - val_mean_squared_error: 0.0917\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0562 - mean_squared_error: 0.0562 - val_loss: 0.0986 - val_mean_squared_error: 0.0986\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0642 - mean_squared_error: 0.0642 - val_loss: 0.0743 - val_mean_squared_error: 0.0743\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0858 - val_mean_squared_error: 0.0858\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0583 - mean_squared_error: 0.0583 - val_loss: 0.1039 - val_mean_squared_error: 0.1039\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0815 - mean_squared_error: 0.0815 - val_loss: 0.0946 - val_mean_squared_error: 0.0946\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0833 - mean_squared_error: 0.0833 - val_loss: 0.1576 - val_mean_squared_error: 0.1576\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0924 - mean_squared_error: 0.0924 - val_loss: 0.0640 - val_mean_squared_error: 0.0640\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0828 - mean_squared_error: 0.0828 - val_loss: 0.1473 - val_mean_squared_error: 0.1473\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0833 - mean_squared_error: 0.0833 - val_loss: 0.0903 - val_mean_squared_error: 0.0903\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0695 - mean_squared_error: 0.0695 - val_loss: 0.0680 - val_mean_squared_error: 0.0680\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0761 - mean_squared_error: 0.0761 - val_loss: 0.1117 - val_mean_squared_error: 0.1117\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0571 - mean_squared_error: 0.0571 - val_loss: 0.0942 - val_mean_squared_error: 0.0942\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0814 - mean_squared_error: 0.0814 - val_loss: 0.1107 - val_mean_squared_error: 0.1107\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0508 - mean_squared_error: 0.0508 - val_loss: 0.1182 - val_mean_squared_error: 0.1182\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - 1s 21ms/sample - loss: 0.0518 - mean_squared_error: 0.0518 - val_loss: 0.1138 - val_mean_squared_error: 0.1138\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0594 - mean_squared_error: 0.0594 - val_loss: 0.0680 - val_mean_squared_error: 0.0680\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0582 - mean_squared_error: 0.0582 - val_loss: 0.1503 - val_mean_squared_error: 0.1503\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - 1s 20ms/sample - loss: 0.0820 - mean_squared_error: 0.0820 - val_loss: 0.1039 - val_mean_squared_error: 0.1039\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    trainingImgdata,\n",
    "    trainingGS,\n",
    "    batch_size=8,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
